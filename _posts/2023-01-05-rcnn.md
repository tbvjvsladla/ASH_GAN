---
layout: post
title:  "RCNN 논문 리뷰"
comments: true
tags: [RCNN]
---

RCNN 논문리뷰
===


Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5)


abstract
---
객체 감지 성능에 있어, `PASCAL VOC`데이터 세트를 활용해 측정한 성능은 몇년간 정체상태였으며, 가장 성능이 좋은 방법은 여러 하위 이미지 feature와 상위레벨의 context를 결합하는 앙상블 시스템을 구성하는 것이다.   
본 논문은 mean average precision(mAP) 측정 지표에 있어 VOC 2012 결과값 대비 30%이상 개선된(mAP가 53.3%로 측정됨) 탐지 알고리즘을 제안한다.   
논문의 접근방식은 아래와 같다.   
1. obj를 segment하고 localize하기 바텀-업 방식의 제안영역에 높은 용량을 갖는 CNNs을 적용할 수 있다
2. 훈련을 위한 데이터가 라벨링이 덜 됬을 때, 사전 지도학습(supervised pre-training)을 보조작업으로 도메인별 파인튜닝을 수행해 성능 향상을 가져올 수 있다.


논문은 이 방법을 R-CNN : Regions with CNN이라 부르려 한다  
제안한 RCNN을 ILSVERC2013에서 발표된 OverFeat랑 비교해보니 성능이 더 좋앗다.
      





introduction
---

기존 시각 인식 작업(visual recognition task)는 지난 10년간 SIFT, HOG사용을 기반으로 하고 있다.
그러나, PASCAL VOC Obj detection성능을 보면 일반적으로 10~12년 동안 앙상블 시스템 구축 프로세스를 기반으로 진행되어왔고 해당 방식으로 조금씩의 진보가 이뤄지고 있었다.   

* SIFT(Scale Invariant Feature Transform)
  * 이미지에서 특징점을 추출하는 대표적 알고리즘 중 하나임.   
이미지의 Scale, Rotation에 불변하는 feature(특징)을 찾아냄.

```python
import numpy as np
import cv2
from matplot import pyplot as plt

img1 = cv2.imread('falsify_img.jpg') #변조된 이미지
img2 = cv2.imread('orign_img.jpg') #원본 이미지

sift = cv2.xfeature2d.SIFT_create()
#SIFT 추출기 생성

kp1, des1 = sift.detectAndCompute(img1, None)
#키 포인트 검출과 서술자 계산 -> 이미지 1에 대한
kp2, des2 = sift.detectAndCompute(img2, None)

bf = cv2.BFMacher(cv2.NORM_HAMMING, crossCheck=Ture)
#매칭을 위한 변수 만들고 초기화

matches = bf.match(des1, des2) #매칭 시작
matches = sorted(matches, key = lambda x:x.distance)
#매칭된 특징점들을 길이별로 정렬함

img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:10], flags=2)
#이미지1, 이미지2의 매칭결과를 키 포인트와 함께 새로 그림을 그려서 보여줌
plt.imshow(img3)
plt.show()
```
대략 아래와 같은 그림이 그려질 것임
![img](../images/43a91beeccc4732597f035ff55f22db2.jpg)

* HOG(Histograms of oriented gradient)
  * 사전 지식으로 edge 검출부터 알아야함   
우선 edge검출의 경우 픽셀의 변화량이 큰 지점을 엣지라 볼 수 있음   
여기서 Histogram of gradient는 이 픽셀의 변화량을 화살표로 표시하여,
화실표를 히스토그램 형태의 feature를 추출하는 방법을 말함

![img2](../images/2093023.png)

  * 이때 화살표를 그리는 방식에 Gradient orientation(화살표 방향), Gradient magnitude(화살표 크기) 두가지 정보를 만들어 낼 수 있음    
이 두가지 정보를 이용해서 HoG를 구하고 이거로 경계면을 추출해 사람이나 차선등을 구분할 수 있음

![img3](../images/wprVEuJ.png)

  * 사전 지식으로 pixels, cells, blocks, windows의 개념에 대해 알아야 한다.   
픽셀(pixels) : 말 그대로 영상 내 하나의 픽셀 값   
셀(Cells) : 픽셀을 몇개 묶어서 그룹화 한 것   
block : 셀을 묶어서 그룹화 한 것   
window : 검출하고자 잘라낸 영역(블록보다는 크다)   

  * 그렇다면 HOG는 보행자 검출을 위해 만들어진 특징 디스크립터라 볼 수 있으며, 이것에 대한 방식으로 기울기 벡터 크기(magnitude)와 방향(direction)을 히스토그램으로 나타내 계산한 것이다.

```python
img = cv2.imread('img.png')
img = np.float(img)

gx = cv2.Sobel(img, cv2.CV_32F, 1, 0)
gy = cv2.Sobel(img, cv2.CV_32F, 0, 1)
#소벨필터를 활용해 32float 데이터 타입으로 x방향, y방향으로 각각 1차미분
magnitude, angle = cv2.cartToPolar(gx, gy)
#앞에서 편미분 결과 gx, gy를 극좌표로 전환 -> 방향벡터의 크기 및 방향 추출
```
위 계산 후 정규화(normalization)과정을 수행하여 윈도에 대한 히스토그램 특징이 계산된다면 이것과 유사한 정규화 값을 결과값은 검출하고자 하는 대상이라 볼 수있다.   
이것을 OpenCV에서는 HOG 디스크립터 계산을 위한 함수를 따로이 제공한다.   


```python
import cv2

#default 디텍터를 위한 HOG 객체 생성 및 설정
hogdef = cv2.HOGDescriptor()
hogdef.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())

#dailer 디텍터를 위한 HOG 객체 생성 및 설정
hogdaim = cv2.HOGDescriptor((48, 96), (16, 16), (8,8), (8,8), 9)
#파라미터 : 윈도 크기, 블록 크기, 정규화 블록 겹침 크기, 셀 크기, 히스토그램 계급 수
hogdaim.setSVMDetector(cv2.HOGDescriptor_getDaimlerPeopleDetector())

cap = cv2.VideoCapture('../img/walking.avi')
mode = True #모드 변환을 위한 플래그 변수
while cap.isOpened():
    ret, img = cap.read()
    if ret :
        if mode:
            # default 디텍터로 보행자 검출
            found, _ = hogdef.detectMultiScale(img)
            for (x,y,w,h) in found:
                cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,255))
        else:
            # daimler 디텍터로 보행자 검출
            found, _ = hogdaim.detectMultiScale(img)
            for (x,y,w,h) in found:
                cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0))
        cv2.putText(img, 'Detector:%s'%('Default' if mode else 'Daimler'), \
                        (10,50 ), cv2.FONT_HERSHEY_DUPLEX,1, (0,255,0),1)
        cv2.imshow('frame', img)
        key = cv2.waitKey(1) 
        if key == 27:
            break
        elif key == ord(' '):
            mode = not mode
    else:
        break
cap.release()
cv2.destroyAllWindows()
```

여기까지가 SIFT, HOG설명이다....
RCNN 논문에서는 SIFT와 HOG가 영장류의 시각경로의 첫번째 피질 영역인 V1세포의 기능과 연관시킬수 있다는 블록별 방향 히스토그램이라 설명하고 있는데... 흠...   

여기서 neocognitron패턴인식 프로세스가 등장(해당 프로세스는 supervised training algorithm이 부재)   
다음으로 합성곱 신경망(convolutional neural network, CNNs)순으로 발전이 이뤄졌다 이야기를 전개하고 있음   

CNN은 1990년도에 처음등장 -> 벡터머신 등장으로 사장되었으나, 12년 Krizhevsky et al.이 ImageNet Large Scale Visual Recognition Challenge(ILSVRC)에서 훨씬 높은 이미지 분류 정확도를 보여주며 다시 관심이 높아짐  
ImageNet의 CNN classification결과가 PASCAL VOC Challenage(파스칼 VOC 챌린지 대회)에서도 적용이 가능할까??

이 논문에서는 그림1과 같이 CNN을 simpler HOG-like와 같이 사용해서 더 높은 객체 감지 성능을 이끌어 낼 수 있을것이라 제안함???(번역이 어려움..)   
아무튼 비교대상은 2010년에 발표된 Pascal VOC대비 자신들이 제안하는 R-CNN의 성능이 더 높다는 것임

![img4](../images/K-001.png)

+ R-CNN동작 개요는 (1)입력 이미지를, (2)2000개의 bottom-up 영역을 추출한 뒤, (3)CNN을 통해 feature를 추출한 다음, (4)
classification과정을 통해 영역별로 객체인식을 진행함   
R-CNN의 mean average precision(mAP)는 53.7%가 나왔음

이미지 분류(classification)와 다르게 이미지 detection(감지)는 검출하고자 하는 객체(object)에 대해 localizing을 수행해야 한다.   
이것에 대한 접근 방식(localization을 수행하기 위한 접근방식)은 regression problem이다.   
+ 회귀분석 : 인과관계를 파악하기 위한 분석방법   
![img5](../images/K-002.png)


이 로컬라이제이션 방식으로 잘 알려진 것은 슬라이딩 윈도우 검출기를 만들어서 사용하는 것이다.   
![img6](../images/images_cha-suyeon_post_826faf40-bf3b-4bec-b823-5ef02dbec51c_image.png)   

CNN은 주로 위 검출기를 활용해서 로컬라이제이션을 먼저 수행하고 있다.   
여기에 사용되는 CNN은 높은 공간 해상도(high spatial resolution)를 유지하기 위해 2개의 컨볼루션 레이어랑 풀링 레이어로만 구성하는 것이 일반적이다.   
+ convolution layer : 합성곱 연산을 통해 이미지의 특징을 추출하는 역할을 수행하는 레이어   
합성곱 연산 방식은 커널혹은 필터라는 n x m행렬로 입력 이미지를 훑으면서 발생한 행렬원소합을 출력화 하는 것이다.   
![img7](../images/K-003.png)   
최종 결과는 feature map(특성맵)이 출력된다.
이때 이미지가 훑는 작업을 한칸, 두칸, 세칸씩으로 지정할 수 있는데 이 이동범위를 stride(스트라이드)라 하고, feature map이 입력 행렬 대비 크기가 작아지는 문제를 방지하기 위해 입력 행렬의 바깥행렬을 추가하는 것을 padding(패딩)이라 하며 통상적으로 패팅은 0으로 값을 채운다
+ Pooling layer : 컨벌루션 레이어를 거친 후에는 풀링 레이어를 추가하는 것이 일반적이며, 풀링 레이어는 feature map를 다운샘플링하여 feature map의 크기를 줄이는 풀링 연산을 수행한다.   
이때, max pooling 혹은 average pooling을 사용한다.   
![img8](../images/maxpooling.png)   

하여 사용된 CNN은 Conv -> 활성화 함수(ReLu) -> Pool -> Conv -> ReLU -> Pool 이런 형식으로 설계된 신경망으로 유추해 볼 수 있다.   
논문 R-CNN의 사용된 CNN은 5개의 conv로 구성되어 있고, 높은 수용 필드(large receptive field), 넓은 strides로 되어 있다   
+ receptive field : 출력 레이어의 뉴런 하나에 영향을 미치는 입력 뉴런들의 공간 크기라고 하는데 컨벌루션 과정에 사용되는 커널(필터)의 크기로 볼 수 있는거 같다.  
+ 논문에서는 필터 크기가 195x195픽셀이고 스트라이드는 32x32픽셀로 사용했다. 입력이미지가 굉장히 큰 이미지인거 같다

논문에서는 로컬라이제이션 문제를 해결하기위해 2000가지 범주의 독립적 영역을 생성하는 방식을 제안하고 CNN을 통해 각 영역에서 고정길이의 특징벡터를 추출한 다음, 각 영역을 범주별로 선형 SVM으로 분류한다.   
+ SVM(Supprot Vector Machine) : 데이터가 어느 카테고리에 속할 지 판단하는 이진 선형 분류 모델   
![img9](../images/sdfs.png)   
위 그림처럼 데이터가 빨간색이랑 파란색이 있을때 가운데 선을 그어서 두 데이터 군집을 나눌때 어케 선을 그려서 나눌지에 대한 모델이라 보면 된다   

obj detection에서 발생하는 두번제 문제는 labeled 된 데이터(라벨링이 된 데이터)가 부족하고, 현재의 train dataset이 CNN을 train하게에 충분하지 않다는 점이다.  
이것에 대한 해결방법은 일반적으로 unsupervised pre-training 후 supervised fine-tuning을 수행하는 것이다.(감독되지 않은 사전 훈련 후 파인 튜닝)   
+ unsupervised pre-training : 음.. 요즘은 잘 안쓰는 방법인듯? 일단 supervised learning은 지도학습이라 불린다   일단 비지도학습은 히든레이어의 레이어 별로 과도하게 학습을 시키는거 같긴함   
[![un-train-img](../images/K-004.png)](https://www.youtube.com/watch?v=Oq38pINmddk&t=735s)   
이거는 잘 모르겠으니 그냥 유투브 영상 링크를 걸어놓는다   




